{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "598a8ff9-aeed-4ad4-a231-ddf50ad8f517",
   "metadata": {},
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "This notebook outlines the steps to prepare the data for various baseline methods. Before running the code, please download the relevant datasets from the provided data space and place them in the 'raw' folder.\n",
    "\n",
    "[Magellan Repository](https://sites.google.com/site/anhaidgroup/useful-stuff/the-magellan-data-repository)\n",
    "\n",
    "[WDC](https://webdatacommons.org/largescaleproductcorpus/v2/)\n",
    "\n",
    "The first part of this notebook outlines the steps to process the raw datasets, while the second part focuses on creating datasets tailored for the corresponding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0cd8e8-f90f-4fde-ae90-b8b8213514c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acddae2-e68d-468d-80f7-1dbf2198ca33",
   "metadata": {},
   "source": [
    "## General Data Preparation\n",
    "\n",
    "We need differnt handling strategy for the Magellan datasets and WDC dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a72c19-0081-4a50-92bb-34bc0e05fb46",
   "metadata": {},
   "source": [
    "### Magellan Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca57b49-4edb-4a7f-b86c-818b0894b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "magellan_dirs = {\n",
    "    'abt': 'raw/abt_buy', 'amgo': 'raw/amazon_google',\n",
    "    'beer': 'raw/beer', 'dbac': 'raw/dblp_acm',\n",
    "    'dbgo': 'raw/dblp_scholar', 'foza': 'raw/fodors_zagat',\n",
    "    'itam': 'raw/itunes_amazon', 'waam': 'raw/walmart_amazon',\n",
    "    'roim': 'raw/rotten_imdb', 'zoye': 'raw/zomato_yelp'\n",
    "}\n",
    "\n",
    "magellan_rename_columns = {\n",
    "    'abt': ['id', 'name', 'description', 'price'], 'amgo': ['id', 'name', 'manufacturer', 'price'],\n",
    "    'beer': ['id', 'name', 'factory', 'style', 'ABV'], 'dbac': ['id', 'title', 'authors', 'venue', 'year'],\n",
    "    'dbgo': ['id', 'title', 'authors', 'venue', 'year'], 'foza': ['id', 'name', 'address', 'city', 'phone', 'type', 'class'],\n",
    "    'itam': ['id', 'name', 'artist', 'album', 'genre', 'price', 'copyright', 'time', 'released'],\n",
    "    'waam': ['id', 'name', 'category', 'brand', 'modelno', 'price'],\n",
    "    'roim': ['id', 'name', 'release date', 'director', 'duration', 'genre'],\n",
    "    'zoye': ['id', 'name', 'votes', 'rating', 'phone', 'address', 'zip', 'cuisine']\n",
    "}\n",
    "\n",
    "magellan_drop_columns = {\n",
    "    'abt': ['description'], 'amgo': [], 'beer': [], 'dbac': [], 'dbgo': [], 'foza': [], 'itam': [],\n",
    "    'waam': [], 'roim': [], 'zoye': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8a7b2d-a080-4532-9722-02fe708c6f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_id(tableA, tableB, id_pairs):\n",
    "    left_merged = pd.merge(tableA, id_pairs, left_on='id', right_on='ltable_id')\n",
    "    left_right_merged = pd.merge(left_merged, tableB, left_on='rtable_id', right_on='id', suffixes=('_l', '_r'))\n",
    "    left_right_merged.drop(columns=['ltable_id', 'rtable_id', 'id_l', 'id_r'], inplace=True)\n",
    "    return left_right_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dbc5ea-e2f4-41db-aab5-0a7cc68b2536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_with_id(tableA, tableB, id_pairs):\n",
    "    left_merged = pd.merge(tableA, id_pairs, left_on='id', right_on='ltable_id')\n",
    "    left_right_merged = pd.merge(left_merged, tableB, left_on='rtable_id', right_on='id', suffixes=('_l', '_r'))\n",
    "    left_right_merged.drop(columns=['ltable_id', 'rtable_id', 'id_l', 'id_r'], inplace=True)\n",
    "    return left_right_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16bbb31-46c8-4047-aa75-e77ea520938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_magellan_row_pairs(dirs: dict, rename_columns: dict, drop_columns: dict):\n",
    "    for d_name in dirs:\n",
    "        tableA = pd.read_csv(os.path.join(dirs[d_name], 'tableA.csv'))\n",
    "        tableB = pd.read_csv(os.path.join(dirs[d_name], 'tableB.csv'))\n",
    "        tableA.columns = rename_columns[d_name]\n",
    "        tableB.columns = rename_columns[d_name]\n",
    "        tableA.drop(columns=drop_columns[d_name], inplace=True)\n",
    "        tableB.drop(columns=drop_columns[d_name], inplace=True)\n",
    "\n",
    "        train_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'train.csv'))\n",
    "        valid_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'valid.csv'))\n",
    "        test_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'test.csv'))\n",
    "        train_df = merge_with_id(tableA, tableB, train_id_pairs)\n",
    "        valid_df = merge_with_id(tableA, tableB, valid_id_pairs)\n",
    "        test_df = merge_with_id(tableA, tableB, test_id_pairs)\n",
    "        if len(test_df) > 1250:\n",
    "            test_df = test_df.sample(n=1250)\n",
    "\n",
    "        if not os.path.exists(f'prepared/{d_name}'):\n",
    "            os.makedirs(f'prepared/{d_name}')\n",
    "        train_df.to_csv(f'prepared/{d_name}/train.csv', index=False)\n",
    "        valid_df.to_csv(f'prepared/{d_name}/valid.csv', index=False)\n",
    "        test_df.to_csv(f'prepared/{d_name}/test.csv', index=False)\n",
    "\n",
    "# prepare_magellan_row_pairs(magellan_dirs, magellan_rename_columns, magellan_drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87cb60-81ef-406c-a8b2-b59e6f26c04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe67121-666d-4492-943d-7e2de43b6abc",
   "metadata": {},
   "source": [
    "### WDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa74a42e-2d87-4720-9309-eccf73d12770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wdc_row_pairs(dir: str):\n",
    "    used_columns = ['title_left', 'price_left', 'priceCurrency_left', 'label', 'title_right', 'price_right', 'priceCurrency_right']\n",
    "    train_df = pd.read_pickle(os.path.join(dir, 'train.pkl.gz'))[used_columns]\n",
    "    valid_df = pd.read_pickle(os.path.join(dir, 'valid.pkl.gz'))[used_columns]\n",
    "    test_df = pd.read_pickle(os.path.join(dir, 'test.pkl.gz'))[used_columns]\n",
    "\n",
    "    merge_price_currency = lambda x, y: str(y) + str(x) if pd.notna(x) and pd.notna(y) else None\n",
    "    train_df['price_left'] = train_df.apply(lambda x: merge_price_currency(x['price_left'], x['priceCurrency_left']), axis=1)\n",
    "    train_df['price_right'] = train_df.apply(lambda x: merge_price_currency(x['price_right'], x['priceCurrency_right']), axis=1)\n",
    "    train_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    train_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "\n",
    "    valid_df['price_left'] = valid_df.apply(lambda x: str(x['price_left'])+ str(x['priceCurrency_left']), axis=1)\n",
    "    valid_df['price_right'] = valid_df.apply(lambda x: str(x['price_right'])+ str(x['priceCurrency_right']), axis=1)\n",
    "    valid_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    valid_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "\n",
    "    test_df['price_left'] = test_df.apply(lambda x: str(x['price_left'])+ str(x['priceCurrency_left']), axis=1)\n",
    "    test_df['price_right'] = test_df.apply(lambda x: str(x['price_right'])+ str(x['priceCurrency_right']), axis=1)\n",
    "    test_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    test_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "    if len(test_df) > 1250:\n",
    "            test_df = test_df.sample(n=1250) \n",
    "\n",
    "    if not os.path.exists(f'prepared/wdc'):\n",
    "        os.makedirs(f'prepared/wdc')\n",
    "    train_df.to_csv(f'prepared/wdc/train.csv', index=False)\n",
    "    valid_df.to_csv(f'prepared/wdc/valid.csv', index=False)\n",
    "    test_df.to_csv(f'prepared/wdc/test.csv', index=False)\n",
    "\n",
    "# prepare_wdc_row_pairs('raw/wdc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961b6d2-41b2-4f35-8c3d-14bcee743d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dadf5c8-da97-4318-b3e7-584d2a36be49",
   "metadata": {},
   "source": [
    "## Method Specific Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc362b8-18b2-4d03-a18c-e005c6ccc40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dataset_dir = 'processed/'\n",
    "datasets = ['abt', 'amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'roim', 'waam', 'wdc', 'zoye']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96801ada-0df8-4288-8b56-4b752840ab58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### OpenAI GPT\n",
    "\n",
    "We need to prepare the prompts that can be used by the OpenAI api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28130a89-5619-4412-ae07-684ebefce8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt-3.5-turbo', 'gpt-4o-mini', 'gpt-4']\n",
    "rand_seeds = [42, 44, 46, 48, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d1f4f8f-c38a-4244-8048-0e7fe44ae70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_msg = '''Determine whether the two entity descriptions refer to the same real-world entity. Answer with 'Yes' if they do and 'No' if they do not.'''\n",
    "zero_shot_request_template = {\"custom_id\": None, \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": None, \"messages\": [{\"role\": \"system\", \"content\": None},{\"role\": \"user\", \"content\": None}],\"max_tokens\": 15}}\n",
    "few_shot_request_template = {\"custom_id\": None, \"method\": \"POST\", \"url\": \"/v1/chat/completions\", \"body\": {\"model\": None, \"messages\": [{\"role\": \"system\", \"content\": None},{\"role\": \"user\", \"content\": None}, {\"role\": \"assistant\", \"content\": None},{\"role\": \"user\", \"content\": None}, {\"role\": \"assistant\", \"content\": None},{\"role\": \"user\", \"content\": None}, {\"role\": \"assistant\", \"content\": None},{\"role\": \"user\", \"content\": None}],\"max_tokens\": 15}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a941482-2bc6-415a-89f4-3c7ba6fa4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serializer(data, rand_seed):\n",
    "    random.seed(rand_seed)\n",
    "    columns = [col[:-2] for col in data.columns if col.endswith('_l')]\n",
    "    random.shuffle(columns)\n",
    "    columns_l = [col+'_l' for col in columns]\n",
    "    columns_r = [col+'_r' for col in columns]\n",
    "    \n",
    "    text_l = '{}\\t' * (len(columns) - 1) + '{}'\n",
    "    text_r = '{}\\t' * (len(columns) - 1) + '{}'\n",
    "    text = f'Entity 1: {text_l}\\nEntity 2: {text_r}\\nYour answer is '\n",
    "    data['text'] = data.apply(lambda x: text.format(*x[columns_l], *x[columns_r]), axis=1)\n",
    "    texts = data['text'].to_list()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228103fa-9fd6-4d28-b5bb-491c51982ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrations(loo_dataset, mode='manual'):\n",
    "    leaved_datasets = [d for d in datasets if d!=loo_dataset]\n",
    "    candidates = []\n",
    "    labels = []\n",
    "    if mode == 'manual':\n",
    "        with open('gpt/demonstrations.json') as f:\n",
    "            examples = json.load(f)\n",
    "        for dataset in leaved_datasets:\n",
    "            candidates += examples[dataset]['mismatches'] + examples[dataset]['matches']\n",
    "            labels += ['No', 'No'] + ['Yes']\n",
    "        for i, candidate in enumerate(candidates):\n",
    "            text_l, text_r = candidate.split('\\n')\n",
    "            candidates[i] = f'Entity 1: {text_l}\\nEntity 2: {text_r}\\nYour answer is '\n",
    "    else:\n",
    "        for dataset in leaved_datasets:\n",
    "            candidates += serializer(pd.read_csv(f'{src_dataset_dir}{dataset}/test.csv'), 42)\n",
    "            labels += pd.read_csv(f'{src_dataset_dir}{dataset}/test.csv')['label'].apply(lambda x: 'Yes' if x==1 else 'No').to_list()\n",
    "    return candidates, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c4088b9-0353-462b-b55e-2f8a5ac12460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_requests(texts, model, dataset, rand_seed, mode='zero-shot'):\n",
    "    requests = []\n",
    "    if mode == 'zero-shot':\n",
    "        for i, text in enumerate(texts):\n",
    "            request = copy.deepcopy(zero_shot_request_template)\n",
    "            custom_id = f'{dataset}-{rand_seed}-{i}'\n",
    "            request['custom_id'] = custom_id\n",
    "            request['body']['model'] = model\n",
    "            request['body']['messages'][0]['content'] = sys_msg\n",
    "            request['body']['messages'][1]['content'] = text\n",
    "            requests.append(request)\n",
    "    else:\n",
    "        if mode == 'few-shot-manual':\n",
    "            candidates, labels = demonstrations(dataset, mode='manual')\n",
    "        elif mode == 'few-shot-random':\n",
    "            candidates, labels = demonstrations(dataset, mode='random')\n",
    "        else:\n",
    "            print('Not supported mode')\n",
    "            return None\n",
    "        for i, text in enumerate(texts):\n",
    "            request = copy.deepcopy(few_shot_request_template)\n",
    "            custom_id = f'{dataset}-{rand_seed}-{i}'\n",
    "            request['custom_id'] = custom_id\n",
    "            request['body']['model'] = model\n",
    "            request['body']['messages'][0]['content'] = sys_msg\n",
    "            id1, id2, id3 = np.random.choice(len(candidates), 3, replace=False)\n",
    "            request['body']['messages'][1]['content'] = candidates[id1]\n",
    "            request['body']['messages'][2]['content'] = labels[id1]\n",
    "            request['body']['messages'][3]['content'] = candidates[id2]\n",
    "            request['body']['messages'][4]['content'] = labels[id2]\n",
    "            request['body']['messages'][5]['content'] = candidates[id3]\n",
    "            request['body']['messages'][6]['content'] = labels[id3]\n",
    "            request['body']['messages'][7]['content'] = text\n",
    "            requests.append(request)\n",
    "\n",
    "    return requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c0f4f8-840d-4cfc-9633-ef0354d751ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gpt_input(model, mode='zero-shot'):\n",
    "    requests = []\n",
    "    for seed in rand_seeds:\n",
    "        for dataset in datasets:\n",
    "            data = pd.read_csv(f'{src_dataset_dir}{dataset}/test.csv')\n",
    "            per_requests = prepare_requests(serializer(data, seed), model, dataset, seed, mode)\n",
    "            requests += per_requests\n",
    "    if mode == 'zero-shot':\n",
    "        file_path = f'gpt/prompts/em-{model}.jsonl'\n",
    "    elif mode == 'few-shot-manual':\n",
    "        file_path = f'gpt/prompts/em-few-shot-manual-{model}.jsonl'\n",
    "    elif mode == 'few-shot-random':\n",
    "        file_path = f'gpt/prompts/em-few-shot-random-{model}.jsonl'\n",
    "    with open(file_path, \"w\") as file:\n",
    "        for item in requests:\n",
    "            file.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3855310d-d0d2-42c9-984a-cf8c3b916871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     prepare_gpt_input(model, 'few-shot-random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c96076-2866-44f9-b48e-7e610feaaf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "277b5afc-7012-4e62-853c-6008172e4ae8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ditto\n",
    "\n",
    "The Ditto accept txt format input, so we need to convert the original csv file into the give txt format,  \"\"\"COL col VAL val...\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "898996fd-8d04-482d-a305-3186b0832bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textualize(data, rand_seed, mode='train'):\n",
    "    random.seed(rand_seed)\n",
    "    columns = [col[:-2] for col in data.columns if col.endswith('_l')]\n",
    "    random.shuffle(columns)\n",
    "    columns_l = [col+'_l' for col in columns]\n",
    "    columns_r = [col+'_r' for col in columns]\n",
    "\n",
    "    if mode == 'test':\n",
    "        text_l = 'COL unknown VAL {} ' * (len(columns) - 1) + 'COL unknown VAL {}'\n",
    "        text_r = 'COL unknown VAL {} ' * (len(columns) - 1) + 'COL unknown VAL {}'\n",
    "        label = '{}'\n",
    "        text = f'{text_l}\\t{text_r}\\t{label}\\n'\n",
    "        data['text'] = data.apply(lambda x: text.format(*x[columns_l], *x[columns_r], x['label']), axis=1)\n",
    "    else:\n",
    "        text_l = 'COL {} VAL {} ' * (len(columns) - 1) + 'COL {} VAL {}'\n",
    "        text_r = 'COL {} VAL {} ' * (len(columns) - 1) + 'COL {} VAL {}'\n",
    "        label = '{}'\n",
    "        text = f'{text_l}\\t{text_r}\\t{label}\\n'\n",
    "        cross_mix = lambda a, b: [item for tup in zip(a, b) for item in tup]\n",
    "        data['text'] = data.apply(lambda x: text.format(*cross_mix(columns, x[columns_l]), *cross_mix(columns, x[columns_r]), x['label']), axis=1)\n",
    "    \n",
    "    texts = data['text'].to_list()\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b0d1d54-6d81-454e-9f03-77c1b3f1234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ditto_dataset(loo_dataset, rand_seed):\n",
    "    leaved_datasets = [d for d in datasets if d!=loo_dataset]\n",
    "    src_train_paths = [f'{src_dataset_dir}{d}/train.csv' for d in leaved_datasets]\n",
    "    src_valid_paths = [f'{src_dataset_dir}{d}/valid.csv' for d in leaved_datasets]\n",
    "    src_test_path = f'{src_dataset_dir}{loo_dataset}/test.csv'\n",
    "\n",
    "    train_texts = []\n",
    "    valid_texts = []\n",
    "    for i in range(len(leaved_datasets)):\n",
    "        train_df = pd.read_csv(src_train_paths[i])\n",
    "        valid_df = pd.read_csv(src_valid_paths[i])\n",
    "        train_texts += textualize(train_df.sample(n=min(1500, len(train_df)), random_state=rand_seed).reset_index(drop=True), rand_seed)\n",
    "        valid_texts += textualize(valid_df.sample(n=min(500, len(valid_df)), random_state=rand_seed).reset_index(drop=True), rand_seed)\n",
    "    test_texts = textualize(pd.read_csv(src_test_path), rand_seed, mode='test')\n",
    "\n",
    "    save_dir = f'ditto/loo-{loo_dataset}-{rand_seed}'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    with open(f'{save_dir}/train.txt', 'w') as f:\n",
    "        f.writelines(train_texts)\n",
    "    with open(f'{save_dir}/valid.txt', 'w') as f:\n",
    "        f.writelines(valid_texts)\n",
    "    with open(f'{save_dir}/test.txt', 'w') as f:\n",
    "        f.writelines(test_texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02c2d42d-de97-4082-b454-82c067d3b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs = []\n",
    "# config_template = {\n",
    "#     \"name\": None,\n",
    "#     \"task_type\": \"classification\",\n",
    "#     \"vocab\": [\"0\", \"1\"],\n",
    "#     \"trainset\": None,\n",
    "#     \"validset\": None,\n",
    "#     \"testset\": None\n",
    "#   }\n",
    "\n",
    "# for seed in rand_seeds:\n",
    "#     for dataset in datasets:\n",
    "#         config = copy.deepcopy(config_template)\n",
    "#         config['name'] = f'loo-{dataset}-{seed}'\n",
    "#         config['trainset'] =  f'../data/ditto/loo-{dataset}-{seed}/train.txt'\n",
    "#         config['validset'] =  f'../data/ditto/loo-{dataset}-{seed}/valid.txt'\n",
    "#         config['testset'] =  f'../data/ditto/loo-{dataset}-{seed}/test.txt'\n",
    "#         configs.append(config)\n",
    "# with open('../ditto/configs.json', \"w\") as file:\n",
    "#     json.dump(configs, file, indent=4)\n",
    "\n",
    "# rand_seeds = [42, 44, 46, 48, 50]\n",
    "\n",
    "# for seed in rand_seeds:\n",
    "#     for dataset in datasets:\n",
    "#         prepare_ditto_dataset(dataset, seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f035538-85a1-474e-a7dc-84975233c550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9897fdb-dafb-4d70-a225-2eab4ff6b811",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Unicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb61fdf8-7ee5-4585-b777-fe8845875187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_unicorn_dataset(dataset, rand_seed):\n",
    "    random.seed(rand_seed)\n",
    "    for partition in ['train.csv', 'valid.csv', 'test.csv']:\n",
    "        df = pd.read_csv(f'{src_dataset_dir}{dataset}/{partition}')\n",
    "        df = df.fillna('')\n",
    "        if partition == 'train.csv':\n",
    "            df = df.sample(n=min(1500, len(df)), random_state=rand_seed).reset_index(drop=True)\n",
    "        elif partition == 'valid.csv':\n",
    "            df = df.sample(n=min(500, len(df)), random_state=rand_seed).reset_index(drop=True)\n",
    "        \n",
    "        left_columns = [col for col in df.columns if col.endswith('_l')]\n",
    "        right_columns = [col for col in df.columns if col.endswith('_r')]\n",
    "        random.shuffle(left_columns)\n",
    "        random.shuffle(right_columns)\n",
    "        item_list = []\n",
    "        for i in range(len(df)):\n",
    "            left_str = '{},'*len(left_columns)\n",
    "            right_str = '{},'*len(right_columns)\n",
    "            left = df.iloc[i][left_columns].values\n",
    "            right = df.iloc[i][right_columns].values\n",
    "            left_str = left_str.format(*left)[:-1]\n",
    "            right_str = right_str.format(*right)[:-1]\n",
    "            item = [left_str, right_str, str(df.iloc[i]['label'])]\n",
    "            item_list.append(item)\n",
    "        if not os.path.exists(f'unicorn/{dataset}-{rand_seed}'):\n",
    "            os.makedirs(f'unicorn/{dataset}-{rand_seed}')\n",
    "        with open(f'unicorn/{dataset}-{rand_seed}/{partition[:-4]}.json', 'w') as f:\n",
    "            json.dump(item_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32c23eb7-339d-4adb-b245-9f9670b57c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset in datasets:\n",
    "#     for seed in rand_seeds:\n",
    "#         prepare_unicorn_dataset(dataset, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76d63f-1b5b-4cef-8caa-d08052b6c255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da935805-9af7-4121-b208-18dd805366d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### AnyMatch\n",
    "AnyMatch requires attribute augmentation and results from autoML, it will be directly placed in the 'processed' folder for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f552b28-653e-4466-aa96-4504c811cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_check(value):\n",
    "    null_strings = [None, 'nan', 'NaN', 'NAN', 'null', 'NULL', 'Null', 'None', 'none', 'NONE', '', '-', '--', '---']\n",
    "    if pd.isna(value) or pd.isnull(value) or value in null_strings:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def numerical_check(value):\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        return 1\n",
    "\n",
    "def string_identical_check(left_value, right_value, row_label):\n",
    "    if left_value == right_value or left_value in right_value or right_value in left_value:\n",
    "        return 1\n",
    "    else:\n",
    "        if row_label == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def numerical_identical_check(left_value, right_value, row_label):\n",
    "    if left_value == right_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def identical_check(left_value, right_value, row_label):\n",
    "    if nan_check(left_value) and not nan_check(right_value):\n",
    "        return 0\n",
    "    elif not nan_check(left_value) and nan_check(right_value):\n",
    "        return 0\n",
    "    elif nan_check(left_value) and nan_check(right_value):\n",
    "        return 1\n",
    "    elif numerical_check(left_value) and numerical_check(right_value):\n",
    "        return numerical_identical_check(left_value, right_value, row_label)\n",
    "    else:\n",
    "        left_value = str(left_value).lower()\n",
    "        right_value = str(right_value).lower()\n",
    "        return string_identical_check(left_value, right_value, row_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789204d1-24cb-4a00-87d3-854982828a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row2attribute_pairs(row):\n",
    "    attr_pairs = []\n",
    "    all_columns = row.index\n",
    "    left_columns = [col for col in all_columns if col.endswith('_l')]\n",
    "    right_columns = [col for col in all_columns if col.endswith('_r')]\n",
    "    row_label = row['label']\n",
    "    for i in range(len(left_columns)):\n",
    "        left_value = row[left_columns[i]]\n",
    "        right_value = row[right_columns[i]]\n",
    "        attr_pair = [left_value, right_value, identical_check(left_value, right_value, row_label), left_columns[i][:-2]]\n",
    "        attr_pairs.append(attr_pair)\n",
    "    return attr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253444c8-e4c2-4d34-91f3-da9220d679c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_attribute_pairs(names: list):\n",
    "    for name in names:\n",
    "        train_row_pairs = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_row_pairs = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "        test_row_pairs = pd.read_csv(f'prepared/{name}/test.csv')\n",
    "        train_attr_pairs = []\n",
    "        valid_attr_pairs = []\n",
    "        test_attr_pairs = []\n",
    "\n",
    "        train_row_pairs.apply(lambda row: train_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "        valid_row_pairs.apply(lambda row: valid_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "        test_row_pairs.apply(lambda row: test_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "\n",
    "        train_attr_pairs_df = pd.DataFrame(train_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        val_attr_pairs_df = pd.DataFrame(valid_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        test_attr_pairs_df = pd.DataFrame(test_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        train_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "        val_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "        test_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        train_attr_pairs_df.to_csv(f'prepared/{name}/attr_train.csv', index=False)\n",
    "        val_attr_pairs_df.to_csv(f'prepared/{name}/attr_valid.csv', index=False)\n",
    "        test_attr_pairs_df.to_csv(f'prepared/{name}/attr_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a46eb17-c20f-46db-9d52-9e77c4dbbb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_all_attribute_pairs(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b2572-bc0e-41b1-b4e2-5f3ed76a08b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_automl_predictions():\n",
    "    dataset_names = ['abt', 'amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'roim', 'waam', 'wdc', 'zoye']\n",
    "    for name in dataset_names:\n",
    "        train_df = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_df = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "\n",
    "        predictor = TabularPredictor(label='label').fit(train_data=train_df, tuning_data=valid_df, verbosity=-1)\n",
    "        train_preds = predictor.predict(train_df)\n",
    "        train_preds_proba = predictor.predict_proba(train_df)\n",
    "        valid_preds = predictor.predict(valid_df)\n",
    "        valid_preds_proba = predictor.predict_proba(valid_df)\n",
    "        train_preds_df = pd.DataFrame({'prediction': train_preds, 'proba_0': train_preds_proba[0], 'proba_1': train_preds_proba[1]})\n",
    "        valid_preds_df = pd.DataFrame({'prediction': valid_preds, 'proba_0': valid_preds_proba[0], 'proba_1': valid_preds_proba[1]})\n",
    "\n",
    "        if not os.path.exists(f'automl/{name}'):\n",
    "            os.makedirs(f'automl/{name}')\n",
    "        train_preds_df.to_csv(f'automl/{name}/train_preds.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925167c-e2fe-4146-a306-acda64256e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare_automl_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef8d2e5-e124-42b2-a36c-2aa9d848e036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf2d9fd-4714-4008-9ddb-102bd44db0b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ZeroER\n",
    "Please just create a seperate 'metadata.txt' file for each dataset, while it include the path of tableA, tableB, and the id pairs of matching entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef3ed2f-c1fe-4bd1-853b-4812c29e7124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f13910e-ae8e-4d57-89f6-df5548fd7862",
   "metadata": {},
   "source": [
    "### Jellyfish & MatchGPT\n",
    "\n",
    "These two methods will directly use data in precessed folder to form prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50acc63-0342-4bbc-8f86-bf81017b6bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
